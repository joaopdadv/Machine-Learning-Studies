{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch # neural network library to test our implementation\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.join(os.getcwd(), 'modules')\n",
    "sys.path.append(module_path)\n",
    "from mlp import MLP, CompoundNN\n",
    "from activation_functions import ReLU, LogSoftmax\n",
    "from losses import MSELoss, NLLLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função para testar método forward em MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_forward_function():\n",
    "    mlp = MLP(6, 5)\n",
    "    x = np.random.randn(1, 6)\n",
    "    out = mlp.forward(x)\n",
    "\n",
    "    mlp_torch = torch.nn.Linear(6, 5) # Linear pois nossa MLP ainda não possui função de ativação\n",
    "    mlp_torch.weight.data = torch.from_numpy(mlp.W).type(torch.float) # Copia pesos\n",
    "    mlp_torch.bias.data = torch.from_numpy(mlp.b).type(torch.float) # Copia bias\n",
    "    out_torch = mlp(torch.from_numpy(x).type(torch.float)) # Calcula saída e transforma para numpy\n",
    "    out_torch = out_torch.data.numpy()\n",
    "\n",
    "    error = ((out - out_torch) ** 2).mean() # Calcula erro médio quadrático\n",
    "    if error < 1e-8:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função para testar Compound Neural Network (Sequencia de MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_compound_nn():\n",
    "    mlp1 = MLP(6,5)\n",
    "    relu1 = ReLU()\n",
    "    mlp2 = MLP(5,4)\n",
    "    relu2 = ReLU()\n",
    "\n",
    "    nn = CompoundNN([mlp1, relu1, mlp2, relu2])\n",
    "\n",
    "    x = np.random.randn(1, 6)\n",
    "\n",
    "    out1 = relu2(mlp2(relu1(mlp1(x))))\n",
    "    out2 = nn(x)\n",
    "\n",
    "    error = ((out1 - out2)**2).mean()\n",
    "\n",
    "    if error < 1e-8:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função para testar save and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_save_and_load():\n",
    "    mlp1 = MLP(6,5)\n",
    "    relu1 = ReLU()\n",
    "    mlp2 = MLP(5,4)\n",
    "    relu2 = ReLU()\n",
    "    nn1 = CompoundNN([mlp1, relu1, mlp2, relu2])\n",
    "\n",
    "    mlp1 = MLP(6,5)\n",
    "    relu1 = ReLU()\n",
    "    mlp2 = MLP(5,4)\n",
    "    relu2 = ReLU()\n",
    "    nn2 = CompoundNN([mlp1, relu1, mlp2, relu2])\n",
    "\n",
    "    x = np.random.randn(1, 6)\n",
    "\n",
    "    nn1.save('nn1')\n",
    "    nn2.load('trainings/nn1')\n",
    "\n",
    "    out1 = nn1(x)\n",
    "    out2 = nn2(x)\n",
    "\n",
    "    error = ((out1 - out2)**2).mean()\n",
    "\n",
    "    if error < 1e-8:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função para testar MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mse_loss():\n",
    "    loss_fct = MSELoss()\n",
    "    loss_fct(np.array([[1, 2]]), np.array([[1.4, 2.5]]))\n",
    "    grad1 = loss_fct.backward()\n",
    "\n",
    "    loss_fct = torch.nn.MSELoss()\n",
    "    x = torch.tensor([[1., 2.]], requires_grad=True)\n",
    "    loss = loss_fct(x, torch.tensor([[1.4, 2.5]]))\n",
    "    loss.backward()\n",
    "    grad2 = x.grad\n",
    "\n",
    "    error = ((grad1 - grad2.data.numpy())**2).mean()\n",
    "\n",
    "    if error < 1e-8:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função para testar gradientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gradients():\n",
    "    target = np.random.randn(1, 5)\n",
    "\n",
    "    mlp = MLP(6, 5)\n",
    "    x = np.random.randn(1, 6)\n",
    "    out = mlp.forward(x)\n",
    "\n",
    "    loss_fct = MSELoss()\n",
    "    loss_fct(out, target)\n",
    "\n",
    "    grad1 = loss_fct.backward()\n",
    "    grad2 = mlp.backward(grad1)\n",
    "    deltaw1 = mlp.deltaW\n",
    "\n",
    "\n",
    "    mlp_torch = torch.nn.Linear(6, 5)\n",
    "    mlp_torch.weight.data = torch.from_numpy(mlp.W).type(torch.float)\n",
    "    mlp_torch.bias.data = torch.from_numpy(mlp.b).type(torch.float)\n",
    "    out_torch = mlp_torch(torch.from_numpy(x).type(torch.float))\n",
    "    out_torch = out_torch\n",
    "\n",
    "    loss_fct = torch.nn.MSELoss()\n",
    "    loss = loss_fct(out_torch, torch.from_numpy(target).type(torch.float))\n",
    "    loss.backward()\n",
    "    deltaw2 = mlp_torch.weight.grad.data.numpy()\n",
    "\n",
    "\n",
    "    error = ((deltaw1.reshape(-1) - deltaw2.reshape(-1)) ** 2).mean()\n",
    "    if error < 1e-8:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função teste para LogSoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_log_softmax():\n",
    "    mlp1 = MLP(6,5)\n",
    "    relu1 = ReLU()\n",
    "    mlp2 = MLP(5,4)\n",
    "    log_softmax = LogSoftmax()\n",
    "    nn = CompoundNN([mlp1, relu1, mlp2, log_softmax])\n",
    "\n",
    "    x = np.random.randn(1, 6)\n",
    "    h = nn(x)\n",
    "    log_probabilites = log_softmax(h)\n",
    "\n",
    "    log_probabilites_torch = torch.nn.functional.log_softmax(torch.from_numpy(h).type(torch.float), dim=1)\n",
    "\n",
    "    error = ((log_probabilites - log_probabilites_torch.numpy()) ** 2).mean()\n",
    "    if error < 1e-8:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função para testar gradiente jacobiana do logsoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_log_softmax_gradient():\n",
    "    x = np.random.randn(1, 6)\n",
    "    log_softmax = LogSoftmax()\n",
    "    log_probabilites = log_softmax(x)\n",
    "    loss_fct = MSELoss()\n",
    "    target = np.random.randn(1, log_probabilites.shape[1])\n",
    "    loss_fct(log_probabilites, target)\n",
    "    grad1 = loss_fct.backward()\n",
    "    grad2 = log_softmax.backward(grad1)\n",
    "\n",
    "    x_torch = torch.from_numpy(x).requires_grad_(True)\n",
    "    log_probabilites_torch = torch.nn.functional.log_softmax(x_torch, dim=1)\n",
    "    loss_fct = torch.nn.MSELoss()\n",
    "    loss = loss_fct(log_probabilites_torch, torch.from_numpy(target))\n",
    "    loss.backward()\n",
    "    deltaw1 = x_torch.grad.data.numpy()\n",
    "    \n",
    "    error = ((grad2 - deltaw1) ** 2).mean()\n",
    "    if error < 1e-8:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_log_softmax_gradient_nllloss():\n",
    "    x = np.random.randn(1, 6)\n",
    "    log_softmax = LogSoftmax()\n",
    "    log_probabilites = log_softmax(x)\n",
    "    loss_fct = NLLLoss()\n",
    "    target = np.array([2])\n",
    "    loss_fct(log_probabilites, target)\n",
    "    grad1 = loss_fct.backward()\n",
    "    grad2 = log_softmax.backward(grad1)\n",
    "\n",
    "    x_torch = torch.from_numpy(x).requires_grad_(True)\n",
    "    log_probabilites_torch = torch.nn.functional.log_softmax(x_torch, dim=1)\n",
    "    loss_fct = torch.nn.NLLLoss()\n",
    "    loss = loss_fct(log_probabilites_torch, torch.from_numpy(target))\n",
    "    loss.backward()\n",
    "    deltaw1 = x_torch.grad.data.numpy()\n",
    "    \n",
    "    error = ((grad2 - deltaw1) ** 2).mean()\n",
    "    if error < 1e-8:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert test_compound_nn() == 0\n",
    "assert test_forward_function() == 0\n",
    "assert test_save_and_load() == 0\n",
    "assert test_mse_loss() == 0\n",
    "assert test_gradients() == 0\n",
    "assert test_log_softmax() == 0\n",
    "assert test_log_softmax_gradient() == 0\n",
    "assert test_log_softmax_gradient_nllloss() == 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
